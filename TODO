We need to implement response timeouts. For instance, after sending a PDU,
we should expect a response pdu (if the protocol defines it) in a particular
interval, after which, we should timeout and consider that request as failed.
A typical example of where the absense of this can result in a problem is during
unbind. If we initiate an unbind, we normally should wait for an unbind_resp{} 
from the SMSC before we shutdown RX. We should not wait for all eternity... ;)


When logging, the gen_server, gen_fsm states are usually included in the log
detail. I've noticed that if I use a the same name for all the states, then the
logs are harder to decipher, as its confusing knowing which module produced the
error. I'll need to rename the state records to reflect which the module
properly so the errors are easier to decipher


Track some statistics:
	- Uptime: Tracked by esme_core
	- txcounter: smpp34_tx
	- rxcounter: smpp34_tcprx
	- ttx: timestamp when pdu is sent off by tx
	- trx1: timestamp when pdu is received by tcprx
	- trx2: timestamp when pdu is received by rx
	- trx3: timestamp when pdu is received by esme_core


Build a disconnected socket interface for handling closing and opening of
sockets at the lower level? Or handle disconnections _in_ the smpp34_esme core
itself? Consider patching gen_tcp too

Apparently, overuse of asynchronous messaging withing the esme_core framework
can cause mailboxes to fill too fast and affect performance. It would seem that
the best thing to do, is to actually use synchronous calls to pass PDUs
among the different esme_core layers, so that the entire stack is never over
loaded with messages. At smpp34_esme there is a queue for storing messages and
in gen_esme34 the user will be expected to implement queing.


Other ideas worth looking into usability-wise:
	- Behave like gen_tcp and use an {active, true|false} operational mechanism
	- Have smpp34_esme as a simple client, exposing {active, true|false} 
      options,while then gen_esme34 as an ESME framework
